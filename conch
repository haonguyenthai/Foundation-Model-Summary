# CONCH (CONtrastive learning from Captions for Histopathology) - Detailed Technical Notes

## Overview and Motivation
CONCH is a visual-language foundation model specifically designed for computational pathology, published in Nature Medicine (March 2024). The model addresses critical limitations in existing computational pathology approaches: (1) label scarcity in medical domains, (2) task-specific model limitations, and (3) underutilization of natural language that is central to pathology practice (reports, journal articles, textbooks).

## Pretraining Dataset Curation

### Data Sources (1.17M human-only pairs, 1.79M total)
- **EDU**: Educational resources with histopathology content
- **PMC-Path**: PubMed Central Open Access Dataset subset
- **Distribution by topic**: 
  - Gastrointestinal tract (121,209), Bones/joints/soft-tissue tumors (111,078), Lung (102,751), Skin (90,585)
  - Liver & biliary tract (89,494), Hematopathology (87,388), CNS (86,163), Female genital tract (83,311)
  - Breast (64,992), Kidney (61,341), and 9 additional categories
- **Stain types**: H&E (457,373), IHC + special stains (713,595)

### Automated Data Cleaning Pipeline (3-step process)
1. **Histopathology Image Detection**: YOLOv5-based object detector trained on synthetic panel data to extract subimages from multi-panel figures
2. **Caption Splitting**: Fine-tuned GPT-style model (pretrained on PubMed) to split captions referring to multiple images into subcaptions
3. **Image-Caption Alignment**: CLIP-based matching model trained on cleaned EDU data to align detected images with split captions using cosine similarity

### Data Filtering Experiments
- **Human-only dataset** (1.17M): Excluded animal histopathology via caption parsing
- **H&E-only dataset** (457K): Additional classifier to identify H&E stains
- **Finding**: Human-only dataset performed best on downstream tasks (Extended Data Fig. 10a)

## Model Architecture

### Base Framework: CoCa (Contrastive Captioners)
State-of-the-art visual-language foundation model adapted for histopathology

### Three Main Components:

#### 1. Image Encoder (θ = θ_backbone + θ_contrast + θ_caption)
- **Backbone**: ViT-base architecture
  - 12 transformer layers, 12 attention heads
  - Embedding dimension: 768, Hidden dimension: 3,072
  - Patch size: 16×16 pixels
  - Input size: 448×448 pixels
  - Learned absolute positional embeddings
  
- **Attentional Pooler #1 (Contrastive)**: 
  - Single query (n_contrast = 1)
  - Outputs 512-dimensional global image representation for alignment
  - Used for zero-shot classification and retrieval
  
- **Attentional Pooler #2 (Captioning)**:
  - 256 queries (n_caption = 256)
  - Outputs 256 image tokens capturing fine-grained details
  - Used for generative tasks (captioning)

#### 2. Text Encoder (ϕ)
- GPT-style autoregressive model with causal attention masks
- 12 transformer layers, embedding dimension 768, hidden dimension 3,072
- Embedding table for token-to-continuous mapping
- Learned absolute positional embeddings
- Appends learned <CLS> token with full context access for global text representation

#### 3. Multimodal Decoder (ψ)
- 12 transformer layers with cross-attention
- Inserts cross-attention layer after each self-attention layer
- Incorporates information from 256 image tokens
- Final language modeling head for next-token prediction

## Pretraining Methodology

### Loss Function (Equal-weighted combination)
```
L = - (1/2M) Σ log[exp(τu_i^T v_i) / Σ exp(τu_i^T v_j)]  [Image-to-Text Contrastive]
    - (1/2M) Σ log[exp(τv_j^T u_j) / Σ exp(τv_j^T u_i)]  [Text-to-Image Contrastive]
    - (1/M) Σ Σ log p(w_{i,t} | w_{i,0:t-1}, x_i; θ,ϕ,ψ)   [Captioning]
```

Where:
- M: mini-batch size
- τ: temperature parameter for contrastive learning
- u_i, v_i: ℓ2-normalized image and text embeddings
- w_{i,t}: token t in caption i

### Training Configuration
- **Hardware**: 8× NVIDIA A100 80GB GPUs with DistributedDataParallel (DDP)
- **Batch size**: Local 48 per GPU, effective global 1,536 (with gradient accumulation)
- **Epochs**: 40
- **Image preprocessing**: Resize shorter edge to 448×448, center crop, zero-pad if needed
- **Optimizer details**: See Supplementary Table 32

### Unimodal Pretraining (Domain-specific initialization)

#### Vision Encoder SSL (iBOT)
- **Data**: 16M image tiles (256×256) at ×20 magnification from 21,442 WSIs
- **Coverage**: 350+ cancer subtypes (OncoTree classification)
- **Method**: State-of-the-art self-supervised learning
- **Hyperparameters**: Supplementary Table 33

#### Language Model Pretraining
- **Data**: 
  - Pathology educational texts
  - 550K+ surgical pathology reports (MGH, deidentified)
  - 400K+ histopathology-relevant PubMed abstracts
- **Architecture**: 24-layer GPT-style autoregressive model
- **Objective**: Next-word prediction (causal language modeling)
- **Initialization**: First 12 layers → text encoder, Last 12 layers → multimodal decoder
- **Hyperparameters**: Supplementary Table 34

#### Ablation Results (Extended Data Fig. 10b)
- Domain-specific vision pretraining >> ImageNet pretraining
- Language pretraining: Similar performance on classification, better on retrieval

## Downstream Evaluation (14 Benchmarks)

### Zero-shot Classification

#### Methodology
1. **Prompt Construction**: 
   - Template: "An image of {class_name}."
   - Multiple templates/synonyms per class → ensemble via mean embedding
   - Embedding generation: Text encoder + <CLS> token (ℓ2-normalized)

2. **Image Embedding**: 
   - Image encoder + contrastive attentional pooler (ℓ2-normalized, 512-dim)

3. **Classification**: 
   - Compute cosine similarity: ŷ_i = argmax_j (u_i^T v_j)
   - Predict class with highest similarity

#### WSI Extension (MI-Zero algorithm)
1. Tile WSI into N patches (256×256 at ×10 equiv. magnification)
2. Compute similarity scores for each tile independently
3. Aggregate via top-K pooling: average K highest scores per class
4. Predict class with highest slide-level score
5. Generate interpretable heatmaps showing tile-level similarities

#### Results Summary (vs PLIP, BiomedCLIP, OpenAICLIP)

**Slide-level tasks:**
- TCGA BRCA (IDC vs ILC): 91.3% (others: 50.7-55.3%, near random) [P<0.01]
- TCGA NSCLC (LUAD vs LUSC): 90.7% (PLIP: 78.7%) [+12.0%, P<0.01]
- TCGA RCC (3-way): 90.2% (PLIP: 80.4%) [+9.8%, P<0.01]
- DHMC LUAD (5 patterns): κ=0.200 (PLIP: 0.080) [+0.12]

**ROI-level tasks:**
- CRC100k (9 tissues): 79.1% (PLIP: 67.4%) [+11.7%, P<0.01]
- WSSS4LUAD (3 tissues): 71.9% (PLIP: 62.4%) [+9.5%, P<0.01]
- SICAP (Gleason grading): Quadratic κ=0.690 (BiomedCLIP: 0.550) [+0.140, P<0.01]

**Key Finding**: Zero-shot performance rivals/exceeds few-shot supervised learning with baselines

### Supervised Classification

#### Methodology
- **Slide-level**: ABMIL (Attention-based Multiple Instance Learning)
  - Maps tile embeddings → 512-dim via FC+ReLU
  - 2-layer gated attention network (hidden dim 384)
  - Dropout 0.25, AdamW optimizer, 20 epochs, lr=1e-4
  - Weighted sampling for class balance
  
- **ROI-level**: Linear probing (Logistic Regression)
  - L-BFGS solver, ℓ2 regularization λ=100/MC
  - Max iterations: 800

#### Baselines
- **Slide-level**: ResNet50 (ImageNet), PLIP, BiomedCLIP, OpenAICLIP
- **ROI-level**: CTransPath (SSL, state-of-the-art), PLIP, BiomedCLIP, OpenAICLIP

#### Results Summary
- **TCGA BRCA**: 86.7% (ResNet50: 76.7%) [+10.0%, P<0.01]
- **TCGA RCC**: 94.2% (ResNet50: 91.6%) [+2.6%]
- **TCGA NSCLC**: 93.3% (ResNet50: 82.6%) [+10.7%, P=0.033]
- **CRC100k**: 93.8% (≈ CTransPath: 93.8%, but >> PLIP/BiomedCLIP by 4-6%)
- **SICAP**: Quadratic κ=0.833 (≈ CTransPath: 0.835, but >> others by 0.071-0.128)

#### Few-shot Learning (Figure 3, Extended Data Fig. 6)
- **Setup**: n_c ∈ {1,2,4,8,16,32,64,128,256,512} labels per class
- **Key Finding**: CONCH requires ~8× fewer labels to match baseline performance
- **Example (BRCA)**: CONCH with 8 labels/class > PLIP/BiomedCLIP with 64 labels/class
- **Zero-shot competitiveness**: CONCH zero-shot > PLIP/BiomedCLIP few-shot (up to 64 labels) on BRCA/NSCLC

### Rare Disease Classification (EBRAINS - 30-way Brain Tumors)

#### Dataset
- 2,319 WSIs from 30 brain tumor subtypes (all rare: <6 per 100K annually)
- Train/Val/Test: 1,151/595/573 slides
- Includes: glioblastomas, meningiomas, pituitary adenomas, lymphomas, etc.

#### Results (Extended Data Fig. 7)
- **Zero-shot**: 37.1% balanced accuracy (BiomedCLIP: 20.1%, random: 3.3%) [+17.0%, P<0.01]
  - Still limited for clinical use → supervised learning needed
  
- **Supervised ABMIL**: 68.2% (CTransPath: 61.4%) [+6.8%, P<0.01]
  - >> PLIP: +10.7%, BiomedCLIP: +14.4%, OpenAICLIP: +17.8% [all P<0.01]
  
- **Few-shot**: CONCH requires ~4× fewer labels for comparable performance

**Interpretation**: Even for challenging 30-class rare disease problems, CONCH provides superior feature representations for supervised learning

### Cross-modal Retrieval

#### Methodology
- **Text-to-Image (t2i)**: Text query → retrieve top-K most similar images
- **Image-to-Text (i2t)**: Image query → retrieve top-K most similar texts
- **Metric**: Recall@K for K ∈ {1,5,10}, plus mean recall (average over K)
- **Similarity**: Cosine similarity in aligned latent space

#### Datasets
- **Source A**: 797 image-caption pairs (held-out source, pathologist-cleaned captions)
- **Source B**: 1,755 image-caption pairs (held-out source)
- **TCGA LUAD**: 165 tiles with in-house pathologist captions (up to 5 per slide)

#### Results Summary (Mean Recall)
**Text-to-Image:**
- Source A: 68.8% (BiomedCLIP: 37.3%) [+31.5%, P<0.01]
- Source B: 39.0% (BiomedCLIP: 23.9%) [+15.1%, P<0.01]
- TCGA LUAD: 24.0% (BiomedCLIP: 18.7%) [+5.3%, P=0.22 but >> PLIP/OpenAICLIP, P<0.01]
- **Average**: 44.0% (BiomedCLIP: 26.7%) [+17.3%, P<0.01]

**Image-to-Text:** Similar trends (Supplementary Tables 22-27)

#### Qualitative Examples
- Complex queries work well (Fig. 4c, Extended Data Fig. 8)
- Examples: "cribriform prostatic adenocarcinoma", "solid-pattern LUAD"
- Retrieved images match morphological descriptions accurately

### Zero-shot Segmentation

#### Methodology
1. Divide WSI into overlapping tiles (224×224, 75% overlap)
2. Classify each tile via zero-shot (cosine similarity)
3. Average predictions in overlapped regions
4. Assign class label to all pixels in each tile
5. Evaluate: Dice score, Precision, Recall (macro-averaged)

#### Datasets & Results
**SICAP (Tumor vs Normal in prostate)** (31 slides)
- Dice: 0.601 (PLIP: 0.549, P=0.08; BiomedCLIP: 0.484, P<0.01)
- Recall: 0.751 (PLIP: 0.644, P<0.01; BiomedCLIP: 0.557, P<0.01)
- Precision: 0.672 (PLIP: 0.605, P=0.024; BiomedCLIP: 0.536, P<0.01)

**DigestPath (Malignant vs Benign in CRC)** (250 slides)
- Dice: 0.615 (PLIP: 0.426, BiomedCLIP: 0.446) [P<0.01 for both]
- Recall: 0.709 (PLIP: 0.541, BiomedCLIP: 0.601) [P<0.01 for both]
- Precision: 0.663 (PLIP: 0.526, BiomedCLIP: 0.581) [P<0.01 for both]

**Finding**: Despite coarse-grained approach, produces reasonable pixel-level masks (Fig. 5d-e)

### Image Captioning (Exploratory)

#### Methodology
- Fine-tune multimodal decoder on Source A (train: 558, val: 77, test: 162)
- Keep only captioning loss (disable contrastive)
- Decoding: Top-K sampling (K=50)
- Baselines: GIT-base, GIT-large (general visual-language models)
- Metrics: METEOR, ROUGE-1

#### Results (Extended Data Fig. 9)
- CONCH outperforms both GIT models (P<0.01)
- Absolute performance limited (small fine-tuning dataset)
- Common issue: Verbatim regurgitation from training data
- Shows potential for generative capabilities with more data

### End-to-end Fine-tuning (Prostate Gleason Grading)

#### Dataset
- 228,482 ROIs (512×512 at ×10): AGGC + PANDA + SICAP
- 4 classes: NC, G3, G4, G5
- Train/Val/Test: 189,484/9,959/29,039 (split at slide level)

#### Baselines
- ViT-B/16 (ImageNet), ViT-L/16 (ImageNet, 3.5× larger)
- ResNet50 (ImageNet)
- CTransPath (SSL, state-of-the-art)
- KimiaNet (DenseNet121, supervised on histopathology)

#### Results (Supplementary Table 31)
**Full dataset:**
- CONCH achieves best performance
- Outperforms even ViT-Large (3.5× parameters)

**Label efficiency (10%, 1% subsampling):**
- CONCH maintains superior performance with limited labels
- Demonstrates robust feature learning from visual-language pretraining

## Technical Implementation Details

### WSI Processing Pipeline
1. **Tissue Segmentation**: CLAM library
   - RGB → HSV color space conversion
   - Binary thresholding on saturation channel
   - Median blurring + morphological closing
   - Contour filtering by area
   
2. **Tiling**: 
   - Classification/supervised: 256×256 at ×10 equiv. (contiguous)
   - Segmentation: 224×224 with 75% overlap at highest magnification
   
3. **Feature Extraction**: 
   - Resize to 224×224
   - Frozen encoder inference
   - Cache embeddings for efficiency

### Prompt Engineering
- **Templates**: Simple, pathologist-curated (Supplementary Tables 35-44)
- **Class names**: Multiple synonyms per class
- **Ensembling**: Mean embedding over templates/synonyms
- **Finding**: Ensembling >> single prompt (Extended Data Fig. 2)
- **Note**: Explicit prompt optimization on validation set could improve further (acknowledged limitation)

### Statistical Analysis
- **Confidence Intervals**: Nonparametric bootstrapping (1000 samples)
- **Significance Testing**: Two-sided paired permutation test (1000 permutations)
- **Null Hypothesis**: No difference in model performance
- **P-value**: Proportion of permuted differences ≥ observed difference (absolute value)

## Key Findings & Contributions

### 1. Scale of Histopathology-specific Data Matters
- Largest histopathology image-caption dataset to date (1.17M pairs)
- Previous work limited by data scarcity (PLIP: Twitter data, BiomedCLIP: ~15M general biomedical pairs)
- Automated curation pipeline enables scalability

### 2. Zero-shot Capabilities Are Clinically Relevant
- Competitive with supervised baselines in few-shot regimes
- Eliminates annotation burden for many classification tasks
- Interpretable heatmaps for pathologist review

### 3. Superior Transfer Learning Across Modalities
- Best-in-class for both vision-only and vision-language tasks
- Single model supports: classification, retrieval, segmentation, captioning
- Outperforms specialized vision-only models (e.g., CTransPath) on many tasks

### 4. Label Efficiency Critical for Rare Diseases
- Requires 4-8× fewer labels than competitors
- Enables practical model development for rare entities
- EBRAINS results demonstrate utility for 30-way fine-grained classification

### 5. Visual-Language Pretraining > Vision-only SSL
- CONCH (visual-language) > CTransPath (vision-only SSL) on slide-level tasks
- Language provides complementary supervisory signal
- Unlocks multimodal capabilities (retrieval, captioning)

## Limitations & Future Directions

### Acknowledged Limitations

1. **Data Scale**: Still ~1000× smaller than general-domain models (LAION-5B)
   - Zero-shot performance on challenging tasks (EBRAINS: 37%) insufficient for clinical use
   - More high-quality image-caption pairs needed

2. **Potential Data Contamination**:
   - Possible overlap between pretraining data (PubMed) and test sets
   - Held out at source level, but not extensively deduplicated
   - Open research question for biomedical domain

3. **Robustness Not Evaluated**:
   - Cross-cohort generalization unclear (different scanners, staining protocols)
   - May require prompt engineering or parameter-efficient fine-tuning

4. **Prompt Sensitivity**:
   - Performance varies with prompt choice (Extended Data Fig. 2)
   - Ensembling helps, but explicit optimization on validation set could improve
   - No longer strictly "zero-shot" if prompts tuned

5. **MI-Zero Limitations**:
   - Best for mutually exclusive class morphologies
   - May fail for tasks requiring: 
     - Both primary/secondary patterns (Gleason scoring)
     - Slide-level positivity from single tumor focus
   - Pooling function requires task-specific adaptation

6. **Subcellular Recognition**:
   - Not evaluated for fine-grained tasks (mitosis detection, cell counting)
   - May require higher-resolution pretraining or specialized architectures

7. **Captioning Performance**:
   - Absolute metrics not ideal (METEOR ~0.17, ROUGE ~0.21)
   - Limited by fine-tuning data scale (558 examples)
   - Verbatim regurgitation issue
   - Requires more high-quality caption data

### Comparison with Concurrent Work

**vs. Recent WSI-to-WSI Matching Study (Reference 66)**:
- CONCH may underperform smaller specialized encoders for specific algorithms
- Highlights importance of continued research on foundation model strengths/limitations
- Suggests foundation models not universally superior for all tasks

### Future Research Directions

1. **Scale up pretraining**:
   - More image-caption pairs (target: 10M+)
   - Higher quality captions (expert curation + synthetic generation)
   - Multi-resolution pretraining

2. **Robustness evaluation**:
   - Cross-scanner, cross-stain, cross-site generalization
   - Domain adaptation techniques for distribution shift

3. **Fine-grained recognition**:
   - Extend to cellular/subcellular tasks
   - Multi-scale architectures

4. **Clinical validation**:
   - Prospective studies in clinical workflows
   - Safety and efficacy assessments
   - Integration with pathologist workflows

5. **Prompt optimization**:
   - Automated prompt search methods
   - Task-specific prompt learning
   - Balance between zero-shot purity and performance

6. **Multimodal integration**:
   - Combine with genomics, radiology, clinical notes
   - Multi-stage reasoning (retrieval → generation)

## Model Release & Reproducibility

### Public Availability
- **Model weights**: https://huggingface.co/MahmoodLab/conch (academic research purposes)
- **Code**: https://github.com/mahmoodlab/CONCH
- **Framework**: PyTorch 2.0.0, CUDA 11.7
- **Key libraries**: 
  - open_clip (version 2.14.0) for CoCa implementation
  - transformers (version 4.27.3) for NLP workflows
  - CLAM for WSI processing

### Reproducibility Details
- All hyperparameters documented (Supplementary Tables 32-37)
- Statistical methods clearly specified (permutation tests, bootstrapping)
- Evaluation datasets publicly available (TCGA, CRC100k, SICAP, etc.)
- In-house data subject to IRB/privacy constraints (aggregate statistics provided)

### Computational Requirements
- **Pretraining**: 8× NVIDIA A100 80GB GPUs
- **Downstream evaluation**: Single NVIDIA 3090 24GB GPU sufficient
- **Inference**: Standard GPU (e.g., V100, A100) for production use

## Clinical & Research Impact

### Potential Applications
1. **Computer-aided diagnosis**: Zero-shot classification for triage/screening
2. **Case retrieval**: Find similar cases for differential diagnosis
3. **Educational tools**: Generate captions for teaching materials
4. **Research cohort building**: Efficient patient selection via retrieval
5. **Quality assurance**: Automated tissue type verification
6. **Rare disease diagnosis**: Few-shot learning from limited examples

### Advantages Over Previous Systems
- **Flexibility**: Single model for multiple tasks vs. task-specific models
- **Efficiency**: Minimal/no fine-tuning required for many applications
- **Interpretability**: Heatmaps and natural language explanations
- **Scalability**: Foundation for open-set recognition (vs. closed-set classifiers)

### Path to Clinical Translation
- Requires: prospective validation, regulatory approval, integration with hospital systems
- Most promising near-term use: Research tools, CAD support (not replacement), education
- Foundation model approach enables rapid adaptation to new tasks/diseases

---

**Reference**: Lu, M.Y., Chen, B., Williamson, D.F.K. et al. A visual-language foundation model for computational pathology. *Nat Med* **30**, 863–874 (2024). https://doi.org/10.1038/s41591-024-02856-4
