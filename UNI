# UNI Model Summary - Architecture & Methods Focus


## Version 2: Detailed Technical Architecture & Methods
*(Comprehensive technical documentation)*

---

## UNI Foundation Model: Architecture, Training Methods & Advantages

### **Core Architecture: Vision Transformer-Large (ViT-L/16)**

#### **Model Specifications**
```
Input: 224×224 RGB patch (H&E stain, ×20 magnification)
Patch Tokenization: 16×16 pixels → 14×14 = 196 tokens per image
Architecture: Plain ViT (non-hierarchical)
  - Embedding dimension: 1,024
  - Transformer layers: 24
  - Attention heads: 16
  - MLP ratio: 4
  - Total parameters: ~304M
Output: [CLS] token from penultimate layer (1,024-D)
Normalization: ImageNet mean/std (RGB)
```

#### **Patch Embedding Layer**
- 2D Convolution: kernel=16×16, stride=16, in_channels=3, out_channels=1024
- Converts 224×224×3 → 14×14×1024 spatial grid
- Flattened to sequence: 196 tokens + 1 [CLS] token = 197 total
- Positional embeddings: Learnable 1D embeddings added to all tokens

#### **Transformer Architecture**
```python
# Per-layer structure (×24 layers)
x = LayerNorm(x)
x = x + MultiHeadSelfAttention(x)  # 16 heads, dim=1024
x = LayerNorm(x)
x = x + MLP(x)  # Hidden dim=4096
```

---

### **Training Methodology: DINOv2 Self-Supervised Learning**

#### **Dual Training Objectives**

**1. Self-Distillation (Knowledge Distillation)**
```
Student Network (UNI) ←→ Teacher Network (EMA of UNI)
```
- Creates two augmented views of same patch (global crops)
- Student predicts teacher's output on masked views
- Teacher updated via exponential moving average (momentum)
- Loss: Cross-entropy between student/teacher [CLS] predictions
- **Advantage:** Learns semantic representations without labels

**2. Masked Image Modeling (MIM)**
```
Original patch → Masked patch (75% tokens hidden)
Student predicts: Teacher's patch token features
```
- Random masking of 75% of patch tokens
- Reconstruction target: Teacher's token-level features (not raw pixels)
- Teacher acts as "online tokenizer"
- **Advantage:** Captures fine-grained spatial features

#### **Training Configuration**
- **Iterations:** 125,000 total
  - First 112,500: 256×256 patches at 0.5 mpp
  - Last 12,500: High-resolution fine-tuning (512×512 patches)
- **Batch Size:** 1,024 per GPU (across 32 A100 GPUs)
- **Optimizer:** AdamW
- **Learning Rate:** Cosine schedule with warmup
- **Data Augmentation:**
  - Global crops: 2 views at scale [0.4, 1.0]
  - Local crops: 10 views at scale [0.05, 0.4]
  - Color jittering, Gaussian blur, solarization

#### **Key DINOv2 Modifications Used**
1. **Untied head weights:** Separate projection heads for distillation vs. MIM
2. **Sinkhorn-Knopp centering:** Prevents representation collapse
3. **KoLeo regularization:** Improves feature diversity across tokens
4. **FlashAttention:** Memory-efficient attention implementation

---

### **Architectural Advantages Over Alternatives**

#### **1. Resolution-Agnostic Design**
**Problem with hierarchical models (Swin Transformers):**
- Fixed spatial downsampling ratios (e.g., 7→14→28→56 tokens)
- Cannot handle arbitrary resolutions without retraining

**UNI's Solution:**
- Plain ViT processes variable grid sizes: N×N tokens
- Positional embedding interpolation:
  ```
  224×224 → 14×14 grid (196 tokens)
  448×448 → 28×28 grid (784 tokens)  [2D interpolation]
  896×896 → 56×56 grid (3,136 tokens)
  1,792×1,792 → 112×112 grid (12,544 tokens)
  ```
- **Benefit:** Adapts to any magnification without architecture modification
- **Validated:** Maintained performance across 224²–1,792² pixels (BACH, UniToPatho datasets)

#### **2. Superior Linear Separability**
**DINOv2 vs. Contrastive Learning (SimCLR, MoCoV3):**
| Method | Linear Probe | Requires Fine-tuning |
|--------|--------------|---------------------|
| MoCoV3 | Moderate | Yes |
| SimCLR | Low | Yes |
| DINOv2 | **High** | **No** |

**Why UNI Excels:**
- Self-distillation creates smoothly structured embedding space
- [CLS] token captures global semantics effectively
- Directly usable for feature extraction without adaptation
- **Validation:** 11 ROI tasks with >80% average balanced accuracy (linear probe)

#### **3. Rich Multi-Scale Features**
**Token-level + Global representations:**
- **196 patch tokens:** Local morphological features (cells, glands, stroma)
- **1 [CLS] token:** Global tissue context (architecture, patterns)
- **Attention maps:** Interpretable focus regions (Fig. 2e, Extended Data Fig. 5-6)

**Demonstrated in paper:**
- High-resolution attention focuses on diagnostically relevant structures
- 1,344² resolution: Fine-grained crypt details for polyp classification
- Automatic morphology discovery without supervision

#### **4. Computational Efficiency**
**Compared to Alternatives:**
| Model | Parameters | Feature Dim | Pre-extraction Speed |
|-------|-----------|-------------|---------------------|
| UNI (ViT-L) | 304M | 1,024 | ~50 patches/sec* |
| REMEDIS (ResNet-152×2) | 232M | 4,096 | ~80 patches/sec |
| CTransPath (Swin-T) | 28M | 768 | ~70 patches/sec |

*Estimated on single NVIDIA 3090 24GB

**Trade-off:** Slightly slower than CNNs, but:
- No need for fine-tuning reduces total compute
- Single forward pass per patch (no multi-crop ensembling)
- Parallelizable across WSI patches

---

### **Key Methodological Advantages**

#### **1. Training Data Diversity: Mass-100K**
**Scale & Coverage:**
- 100M patches from 100K slides (vs. 15M from 29K slides in competitors)
- 20 tissue types (vs. 25-32 cancer types only in TCGA-based models)
- **Includes:** Normal tissue, benign pathologies, rare cancers
- **Sources:** BWH, MGH, GTEx → reduces site-specific bias

**Result:** Better generalization to unseen tissue types and rare diseases
- +19.6% over REMEDIS on 30-class brain tumor subtypes (EBRAINS)
- +16.1% on rare cancer OncoTree classification (OT-108)

#### **2. Few-Shot Learning Efficiency**
**SimpleShot Framework Compatibility:**
```
Class Prototype = Average(feature_vectors per class)
Query Classification = Nearest Centroid
```

**UNI's advantage:**
- High-quality embeddings enable accurate prototypes with <8 examples
- **ROI-level:** 8-shot UNI = 256-shot baseline performance
- **Slide-level (MI-SimpleShot):** 4-shot UNI > 32-shot MIL with other encoders

**Mechanism:** DINOv2's clustering in embedding space
- Similar morphologies naturally cluster
- Class prototypes are stable even with few samples

#### **3. Plug-and-Play Feature Extractor**
**No fine-tuning required for most tasks:**
```
Workflow: WSI → UNI encoder → Frozen features → Lightweight head
```

**Downstream task adaptations:**
- **Slide classification:** ABMIL (2-layer MLP with attention pooling)
- **ROI classification:** Linear classifier (single fully connected layer)
- **Segmentation:** Mask2Former head (only decoder trained)
- **Retrieval:** Direct L2 distance (no training)

**Comparison:**
- Other models often need end-to-end fine-tuning
- UNI: Train only task head (~1M parameters vs. 300M+ full model)
- **Benefit:** 10-100× less compute for deployment

---

### **Architectural Design Choices & Rationale**

#### **Why ViT-L over ViT-B or ViT-H?**
**Tested in paper (Supplementary Tables 13, 16):**
- **ViT-B (86M params):** Good performance, but saturates with more data
- **ViT-L (304M params):** Optimal balance of capacity vs. efficiency
- **ViT-H (632M params):** Not tested (compute constraints)

**Scaling trends observed:**
- Mass-1K → Mass-22K: +4.2% top-1 accuracy (ViT-L)
- Mass-22K → Mass-100K: +3.7% additional gain
- **Conclusion:** ViT-L has sufficient capacity for 100M patch dataset

#### **Why Plain ViT over Hierarchical (Swin)?**
**Trade-offs:**
| Aspect | Plain ViT (UNI) | Swin Transformer |
|--------|-----------------|------------------|
| Dense prediction | Moderate | **Excellent** |
| Transfer learning | **Excellent** | Good |
| Resolution flexibility | **Excellent** | Limited |
| Training stability | **Good** | Requires careful tuning |
| Feature quality | **Excellent** | Good |

**UNI's rationale:**
- Pathology tasks are primarily classification/retrieval (not dense segmentation)
- Transfer learning is critical (34 diverse tasks)
- Segmentation performance still competitive (0.721 Dice on SegPath)

#### **Why [CLS] token from penultimate layer?**
- Final layer may be too specialized to pretraining task
- Penultimate layer balances:
  - **Specificity:** Captures meaningful features
  - **Generality:** Not overfitted to self-supervised proxy task
- Standard practice in ViT literature (DINO, DINOv2, MAE)

---

### **Implementation Details**

#### **Feature Extraction Pipeline**
```python
# Pseudo-code for UNI inference
import torch
from torchvision import transforms

# 1. Image preprocessing
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet stats
        std=[0.229, 0.224, 0.225]
    )
])

# 2. Model forward pass
patch = transform(pil_image)  # 3×224×224
with torch.no_grad():
    features = model.forward_features(patch)  # 197×1024
    cls_token = features[:, 0, :]  # 1×1024
    patch_tokens = features[:, 1:, :]  # 196×1024 (optional)

# 3. Use features downstream
# - Slide-level: Aggregate with ABMIL
# - ROI-level: Pass to linear classifier
# - Retrieval: L2 distance between cls_tokens
```

#### **Memory & Speed**
**Single patch inference:**
- GPU memory: ~2GB (forward pass only)
- Throughput: ~50-100 patches/sec (batch=32, single GPU)

**WSI processing:**
- Average WSI: ~10,000 patches (×20 magnification, 256×256 stride)
- Time: ~3-5 minutes per WSI (feature extraction)
- Storage: ~40MB per WSI (1,024-D × 10K patches × float16)

---

### **Summary of Architectural Strengths**

✅ **Plain ViT design:** Resolution-agnostic, simple, transferable  
✅ **DINOv2 training:** Strong linear separability without fine-tuning  
✅ **Large-scale pretraining:** 100M diverse patches improve generalization  
✅ **Dual objectives:** Semantic (distillation) + spatial (MIM) features  
✅ **Efficient deployment:** Frozen features + lightweight heads  
✅ **Multi-scale capability:** Single model handles 224²–1,792² inputs  
✅ **Interpretable:** Attention maps reveal diagnostic reasoning  
✅ **Few-shot ready:** High-quality embeddings enable prototypical learning  

---

### **When to Use UNI vs. Alternatives**

**Use UNI when:**
- Need transfer learning to new pathology tasks
- Limited labeled data (few-shot scenarios)
- Require resolution flexibility (multi-magnification)
- Want interpretable attention maps
- Deploying across diverse tissue types

**Consider alternatives when:**
- Task requires dense per-pixel predictions (use Swin + full fine-tuning)
- Real-time inference is critical (use lighter ResNet models)
- Domain is extremely narrow (task-specific pretraining may suffice)
